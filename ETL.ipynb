{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a9c7313",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'e:\\\\DATA ANALIST\\\\Git\\\\Proyecto Despliegue\\\\MIAD---PROYECTO---DESPLIEGUE-DE-SOLUCIONES\\\\Data\\\\Catálogo_Nacional_de_Estaciones_del_IDEAM_20251024.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mdir\u001b[39m = os.getcwd()\n\u001b[32m      6\u001b[39m coffe_cities = pd.read_excel(os.path.join(\u001b[38;5;28mdir\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mData\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmunicipios_seleccionados.xlsx\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m stations = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mData\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCatálogo_Nacional_de_Estaciones_del_IDEAM_20251024.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dayan\\miniconda3\\envs\\ia_cluster\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dayan\\miniconda3\\envs\\ia_cluster\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dayan\\miniconda3\\envs\\ia_cluster\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dayan\\miniconda3\\envs\\ia_cluster\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Dayan\\miniconda3\\envs\\ia_cluster\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'e:\\\\DATA ANALIST\\\\Git\\\\Proyecto Despliegue\\\\MIAD---PROYECTO---DESPLIEGUE-DE-SOLUCIONES\\\\Data\\\\Catálogo_Nacional_de_Estaciones_del_IDEAM_20251024.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dir = os.getcwd()\n",
    "\n",
    "coffe_cities = pd.read_excel(os.path.join(dir, 'Data', 'municipios_seleccionados.xlsx'))\n",
    "stations = pd.read_csv(os.path.join(dir, 'Data', 'Catálogo_Nacional_de_Estaciones_del_IDEAM_20251024.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d43da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir por Municipio\n",
    "stations_selected = pd.merge(\n",
    "    coffe_cities[['municipio']],                # municipios seleccionados\n",
    "    stations,\n",
    "    left_on='municipio',\n",
    "    right_on='Municipio',\n",
    "    how='inner'                                 # coincidencias exactas\n",
    ")\n",
    "stations_selected = stations_selected[['Departamento','Municipio','Codigo','Nombre','Categoria','Tecnologia','LONGITUD','LATITUD','Altitud','Fecha_instalacion','Fecha_suspension']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccbf51d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 172 entries, 0 to 171\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Departamento       172 non-null    object \n",
      " 1   Municipio          172 non-null    object \n",
      " 2   Codigo             172 non-null    int64  \n",
      " 3   Nombre             172 non-null    object \n",
      " 4   Categoria          172 non-null    object \n",
      " 5   Tecnologia         172 non-null    object \n",
      " 6   LONGITUD           172 non-null    float64\n",
      " 7   LATITUD            172 non-null    float64\n",
      " 8   Altitud            172 non-null    object \n",
      " 9   Fecha_instalacion  171 non-null    object \n",
      " 10  Fecha_suspension   55 non-null     object \n",
      "dtypes: float64(2), int64(1), object(8)\n",
      "memory usage: 14.9+ KB\n"
     ]
    }
   ],
   "source": [
    "stations_selected.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Parsear las fechas (maneja strings variados; 'errors=\"coerce\"' pone NaT si no se puede parsear)\n",
    "for col in ['Fecha_instalacion', 'Fecha_suspension']:\n",
    "    if col in stations_selected.columns:\n",
    "        stations_selected[col] = pd.to_datetime(\n",
    "            stations_selected[col],\n",
    "            errors='coerce',\n",
    "            dayfirst=True\n",
    "        )\n",
    "\n",
    "# 2) Definir fecha de corte\n",
    "corte = pd.Timestamp('2010-01-01')\n",
    "\n",
    "# 3) Construir la máscara:\n",
    "#    - Mantener registros sin fecha de suspensión (NaT)  -> estación activa o sin dato\n",
    "#    - Mantener registros con suspensión >= 2010-01-01\n",
    "mask = stations_selected['Fecha_suspension'].isna() | (stations_selected['Fecha_suspension'] >= corte)\n",
    "\n",
    "# 4) Aplicar filtro\n",
    "antes = len(stations_selected)\n",
    "stations_selected = stations_selected.loc[mask].reset_index(drop=True)\n",
    "despues = len(stations_selected)\n",
    "\n",
    "print(f\"Estaciones eliminadas por suspensión antes de 2010-01-01: {antes - despues}\")\n",
    "stations_selected.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48df191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo detectado: Catalogo_Nacional_de_Estaciones_del_IDEAM_20251024.csv\n",
      "Estaciones eliminadas por suspensión antes de 2010-01-01: 43\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 129 entries, 0 to 128\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   Departamento       129 non-null    object        \n",
      " 1   Municipio          129 non-null    object        \n",
      " 2   Codigo             129 non-null    int64         \n",
      " 3   Nombre             129 non-null    object        \n",
      " 4   Categoria          129 non-null    object        \n",
      " 5   Tecnologia         129 non-null    object        \n",
      " 6   LONGITUD           129 non-null    float64       \n",
      " 7   LATITUD            129 non-null    float64       \n",
      " 8   Altitud            129 non-null    object        \n",
      " 9   Fecha_instalacion  128 non-null    datetime64[ns]\n",
      " 10  Fecha_suspension   12 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](2), float64(2), int64(1), object(6)\n",
      "memory usage: 11.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "# --- utilidades ---\n",
    "def strip_accents(s: str) -> str:\n",
    "    if s is None: \n",
    "        return \"\"\n",
    "    return \"\".join(ch for ch in unicodedata.normalize(\"NFD\", s) if unicodedata.category(ch) != \"Mn\")\n",
    "\n",
    "def find_station_file(data_dir: Path) -> Path | None:\n",
    "    \"\"\"\n",
    "    Busca un archivo que parezca el catálogo de estaciones del IDEAM, \n",
    "    tolerando acentos y variaciones. Prefiere .csv si hay varias coincidencias.\n",
    "    \"\"\"\n",
    "    target_tokens = [\"catalogo\", \"catálogo\", \"nacional\", \"estaciones\", \"ideam\", \"20251024\"]\n",
    "    # Normaliza tokens sin acentos para comparar\n",
    "    target_tokens = [strip_accents(t).casefold() for t in target_tokens]\n",
    "\n",
    "    candidates = list(data_dir.glob(\"*\"))\n",
    "    scored = []\n",
    "    for p in candidates:\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        name_norm = strip_accents(p.name).casefold()\n",
    "        score = sum(tok in name_norm for tok in target_tokens)\n",
    "        if score >= 3:  # umbral razonable\n",
    "            scored.append((score, p))\n",
    "\n",
    "    if not scored:\n",
    "        return None\n",
    "\n",
    "    # Ordena por score (desc), prefiriendo .csv\n",
    "    scored.sort(key=lambda t: (t[0], t[1].suffix.lower() == \".csv\"), reverse=True)\n",
    "\n",
    "    # Si hay varios, prioriza el .csv con mayor score\n",
    "    best = None\n",
    "    for _, p in scored:\n",
    "        if p.suffix.lower() == \".csv\":\n",
    "            best = p\n",
    "            break\n",
    "    if not best:\n",
    "        best = scored[0][1]\n",
    "    return best\n",
    "\n",
    "# --- rutas base ---\n",
    "cwd = Path(os.getcwd())\n",
    "data_dir = cwd / \"Data\"\n",
    "\n",
    "# Carga municipios\n",
    "coffe_cities_path = data_dir / \"municipios_seleccionados.xlsx\"\n",
    "coffe_cities = pd.read_excel(coffe_cities_path)\n",
    "\n",
    "# Localiza el archivo de estaciones de forma robusta\n",
    "stations_path = find_station_file(data_dir)\n",
    "if stations_path is None:\n",
    "    # Si no se encontró, imprime listado para inspección\n",
    "    print(\"No encontré el catálogo. Archivos en Data/:\")\n",
    "    for p in sorted(data_dir.glob(\"*\")):\n",
    "        print(\" -\", p.name)\n",
    "    raise FileNotFoundError(\"No se encontró el archivo del catálogo IDEAM (revisa nombre/extensión).\")\n",
    "\n",
    "print(f\"Archivo detectado: {stations_path.name}\")\n",
    "\n",
    "# Carga robusta (CSV o Excel)\n",
    "if stations_path.suffix.lower() == \".csv\":\n",
    "    # Intento 1: inferencia de separador con engine=python\n",
    "    tried = []\n",
    "    try:\n",
    "        stations = pd.read_csv(stations_path, sep=None, engine=\"python\")\n",
    "    except Exception as e1:\n",
    "        tried.append(f\"sep=None utf-8: {e1}\")\n",
    "        # Intento 2: codificaciones comunes\n",
    "        loaded = False\n",
    "        for enc in [\"utf-8-sig\", \"latin1\", \"cp1252\"]:\n",
    "            for sep in [\",\", \";\", \"\\t\", \"|\"]:\n",
    "                try:\n",
    "                    stations = pd.read_csv(stations_path, encoding=enc, sep=sep)\n",
    "                    loaded = True\n",
    "                    print(f\"Cargado con encoding={enc} sep='{sep}'\")\n",
    "                    break\n",
    "                except Exception as e2:\n",
    "                    tried.append(f\"encoding={enc} sep='{sep}': {e2}\")\n",
    "            if loaded:\n",
    "                break\n",
    "        if not loaded:\n",
    "            print(\"Intentos fallidos:\\n - \" + \"\\n - \".join(tried))\n",
    "            raise\n",
    "else:\n",
    "    # Excel (por si el “CSV” realmente era xlsx/xls)\n",
    "    stations = pd.read_excel(stations_path)\n",
    "\n",
    "# --- Unir por municipio ---\n",
    "stations_selected = pd.merge(\n",
    "    coffe_cities[['municipio']],\n",
    "    stations,\n",
    "    left_on='municipio',\n",
    "    right_on='Municipio',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# --- Selección de columnas si existen (ignora las que no existan) ---\n",
    "cols_pref = ['Departamento','Municipio','Codigo','Nombre','Categoria','Tecnologia',\n",
    "             'LONGITUD','LATITUD','Altitud','Fecha_instalacion','Fecha_suspension']\n",
    "cols_exist = [c for c in cols_pref if c in stations_selected.columns]\n",
    "stations_selected = stations_selected[cols_exist].copy()\n",
    "\n",
    "# --- Parseo robusto de fechas ---\n",
    "for col in ['Fecha_instalacion', 'Fecha_suspension']:\n",
    "    if col in stations_selected.columns:\n",
    "        stations_selected[col] = pd.to_datetime(stations_selected[col], errors='coerce', dayfirst=True)\n",
    "\n",
    "# --- Filtrar: eliminar suspendidas antes de 2010-01-01 ---\n",
    "corte = pd.Timestamp('2010-01-01')\n",
    "antes = len(stations_selected)\n",
    "# Mantener registros sin suspensión o con suspensión >= corte\n",
    "mask_keep = stations_selected['Fecha_suspension'].isna() | (stations_selected['Fecha_suspension'] >= corte)\n",
    "stations_selected = stations_selected.loc[mask_keep].reset_index(drop=True)\n",
    "eliminadas = antes - len(stations_selected)\n",
    "\n",
    "print(f\"Estaciones eliminadas por suspensión antes de 2010-01-01: {eliminadas}\")\n",
    "print(stations_selected.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83cef8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Categoria  Estaciones  Porcentaje\n",
      "0           Pluviométrica          46       35.66\n",
      "1     Climática Principal          27       20.93\n",
      "2            Limnimétrica          16       12.40\n",
      "3     Climática Ordinaria          13       10.08\n",
      "4            Limnigráfica          10        7.75\n",
      "5           Pluviográfica           9        6.98\n",
      "6  Meteorológica Especial           6        4.65\n",
      "7       Agrometeorológica           1        0.78\n",
      "8     Sinóptica Principal           1        0.78\n"
     ]
    }
   ],
   "source": [
    "# --- Conteo de estaciones por Categoria ---\n",
    "if 'Categoria' not in stations_selected.columns:\n",
    "    raise KeyError(\"La columna 'Categoria' no existe en stations_selected.\")\n",
    "\n",
    "categoria_counts = (\n",
    "    stations_selected['Categoria']\n",
    "    .value_counts(dropna=False)\n",
    "    .rename_axis('Categoria')\n",
    "    .reset_index(name='Estaciones')\n",
    "    .sort_values('Estaciones', ascending=False)\n",
    ")\n",
    "categoria_counts['Porcentaje'] = (categoria_counts['Estaciones'] / categoria_counts['Estaciones'].sum() * 100).round(2)\n",
    "\n",
    "print(categoria_counts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia_cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
