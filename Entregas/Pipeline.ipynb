{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea216d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import re\n",
    "import json, warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e6b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando df_modelo desde: artifacts\\_cache\\df_modelo.parquet\n",
      "Dataset_modelo.csv reconstruido en ./artifacts/\n",
      "Columnas: ['fecha', 'rendimiento', 'NDVI', 'EVI', 'Precipitacion', 'TempMax', 'TempMin', 'HumedadRelativa', 'month_sin', 'month_cos']\n",
      "Filas: 96\n"
     ]
    }
   ],
   "source": [
    "# === 1) Fuente de datos ===\n",
    "\n",
    "# Intenta cargar df_modelo desde varios candidatos\n",
    "def load_df_modelo():\n",
    "    candidates = [\n",
    "        Path(\"artifacts/_cache/df_modelo.parquet\"),\n",
    "        Path(\"artifacts/_cache/df_modelo.csv\"),\n",
    "        Path(\"Data/df_modelo.parquet\"),\n",
    "        Path(\"Data/df_modelo.csv\"),\n",
    "    ]\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            print(f\"Cargando df_modelo desde: {p}\")\n",
    "            if p.suffix == \".parquet\":\n",
    "                return pd.read_parquet(p)\n",
    "            return pd.read_csv(p)\n",
    "    raise FileNotFoundError(\n",
    "        \"No encontré df_modelo en disco. \"\n",
    "    )\n",
    "\n",
    "df_raw = load_df_modelo()\n",
    "\n",
    "\n",
    "# === 2) Normalización de nombres de columnas ===\n",
    "def norm(s):\n",
    "    s = str(s).strip()\n",
    "    s = s.replace(\"á\",\"a\").replace(\"é\",\"e\").replace(\"í\",\"i\").replace(\"ó\",\"o\").replace(\"ú\",\"u\")\n",
    "    s = re.sub(r\"[^a-zA-Z0-9_]\", \"\", s)\n",
    "    return s.lower()\n",
    "\n",
    "ren_map = {c: norm(c) for c in df_raw.columns}\n",
    "df = df_raw.rename(columns=ren_map)\n",
    "\n",
    "# Alias hacia nombres estándar (sin acentos y camel-case)\n",
    "ALIASES = {\n",
    "    \"fecha\": [\"fecha\",\"date\",\"periodo\",\"mes\",\"yyyymm\"],\n",
    "    \"rendimiento\": [\"rendimiento\",\"yield\",\"t_ha\",\"tha\"],\n",
    "    \"ndvi\": [\"ndvi\"],\n",
    "    \"evi\": [\"evi\"],\n",
    "    \"Precipitacion\": [\"precipitacion\",\"ppt\",\"lluvia\",\"rain\"],\n",
    "    \"TempMax\": [\"tempmax\",\"tmax\",\"temp_max\"],\n",
    "    \"TempMin\": [\"tempmin\",\"tmin\",\"temp_min\"],\n",
    "    \"HumedadRelativa\": [\"humedadrelativa\",\"hr\",\"humedad\",\"rh\"]\n",
    "}\n",
    "\n",
    "def map_to_std(df, aliases):\n",
    "    out = {}\n",
    "    for std, cands in aliases.items():\n",
    "        for c in cands:\n",
    "            if c in df.columns:\n",
    "                out[std] = c\n",
    "                break\n",
    "    return out\n",
    "\n",
    "colmap = map_to_std(df, ALIASES)\n",
    "# Promociona a nombres estándar\n",
    "for std, src in colmap.items():\n",
    "    if std != src:\n",
    "        df[std] = df[src]\n",
    "\n",
    "# === 3) Validaciones mínimas ===\n",
    "req_min = {\"fecha\",\"rendimiento\"}\n",
    "assert req_min.issubset(df.columns), f\"Faltan columnas mínimas {req_min - set(df.columns)}\"\n",
    "\n",
    "# === 4) Preparación de fecha y estacionalidad ===\n",
    "if \"fecha\" in df.columns:\n",
    "    # si vino yyyymm en la misma columna ya mapeada a 'fecha'\n",
    "    # intenta parsear primero como YYYY-MM\n",
    "    df[\"fecha\"] = pd.to_datetime(df[\"fecha\"], errors=\"coerce\", format=\"%Y-%m\")\n",
    "    # si quedó NaT, intenta parseo genérico\n",
    "    nan_mask = df[\"fecha\"].isna()\n",
    "    if nan_mask.any():\n",
    "        df.loc[nan_mask, \"fecha\"] = pd.to_datetime(df.loc[nan_mask, \"fecha\"], errors=\"coerce\")\n",
    "else:\n",
    "    raise ValueError(\"No se pudo construir 'fecha'.\")\n",
    "\n",
    "df[\"rendimiento\"] = pd.to_numeric(df[\"rendimiento\"], errors=\"coerce\")\n",
    "\n",
    "df[\"month\"] = df[\"fecha\"].dt.month\n",
    "df[\"month_sin\"] = np.sin(2*np.pi*df[\"month\"]/12.0)\n",
    "df[\"month_cos\"] = np.cos(2*np.pi*df[\"month\"]/12.0)\n",
    "\n",
    "# === 5) Selección de columnas finales (las que existan) ===\n",
    "CAND = [\"NDVI\",\"EVI\",\"Precipitacion\",\"TempMax\",\"TempMin\",\"HumedadRelativa\",\"month_sin\",\"month_cos\"]\n",
    "present = []\n",
    "for c in CAND:\n",
    "    # están con camel-case exacto; crea si existe en lower estándar\n",
    "    if c in df.columns:\n",
    "        present.append(c)\n",
    "    elif c.lower() in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c.lower()], errors=\"coerce\")\n",
    "        present.append(c)\n",
    "\n",
    "# Garantizar tipos numéricos\n",
    "for c in present:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Dataset de salida para tablero (orden recomendado)\n",
    "cols_out = [\"fecha\",\"rendimiento\"] + present\n",
    "df_out = df[cols_out].sort_values(\"fecha\").reset_index(drop=True)\n",
    "\n",
    "# === 6) Guardar dataset_modelo.csv canónico ===\n",
    "ARTI = Path(\"artifacts\"); ARTI.mkdir(exist_ok=True)\n",
    "df_out.to_csv(ARTI / \"dataset_modelo.csv\", index=False)\n",
    "\n",
    "print(\"Dataset_modelo.csv reconstruido en ./artifacts/\")\n",
    "print(\"Columnas:\", df_out.columns.tolist())\n",
    "print(\"Filas:\", len(df_out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42aa22c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Artefactos actualizados en ./artifacts/:\n",
      "   - modelo_boyaca.pkl\n",
      "   - feature_cols.pkl\n",
      "   - metadata.json\n",
      "   - dataset_modelo.csv\n",
      "   - df_resultados.csv\n"
     ]
    }
   ],
   "source": [
    "# === 1) Cargar SIEMPRE el dataset recién construido ===\n",
    "ARTI = Path(\"artifacts\"); ARTI.mkdir(exist_ok=True)\n",
    "df = pd.read_csv(ARTI / \"dataset_modelo.csv\", parse_dates=[\"fecha\"])\n",
    "\n",
    "assert {\"fecha\",\"rendimiento\"}.issubset(df.columns)\n",
    "y = df[\"rendimiento\"].astype(float)\n",
    "\n",
    "CAND = [\"NDVI\",\"EVI\",\"Precipitacion\",\"TempMax\",\"TempMin\",\"HumedadRelativa\",\"month_sin\",\"month_cos\"]\n",
    "feature_cols = [c for c in CAND if c in df.columns]\n",
    "X = df[feature_cols].copy()\n",
    "for c in feature_cols:\n",
    "    X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "\n",
    "# === 2) Split temporal (sin shuffle) + candidatos de modelo ===\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "prepro = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\" , StandardScaler())\n",
    "])\n",
    "\n",
    "candidatos = {\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"Regresión Lineal\": LinearRegression(),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "def evalua(nombre, est):\n",
    "    pipe = Pipeline([(\"prepro\", prepro), (\"model\", est)])\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    pred = pipe.predict(X_te)\n",
    "    mae = mean_absolute_error(y_te, pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_te, pred))\n",
    "    r2 = r2_score(y_te, pred)\n",
    "    return {\"MODELO\": nombre, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2, \"pipeline\": pipe}\n",
    "\n",
    "res = [evalua(n, m) for n, m in candidatos.items()]\n",
    "df_resultados = pd.DataFrame([{k:v for k,v in r.items() if k!=\"pipeline\"} for r in res]).sort_values(\"R2\", ascending=False)\n",
    "\n",
    "best_name = df_resultados.iloc[0][\"MODELO\"]\n",
    "best_pipe = [r[\"pipeline\"] for r in res if r[\"MODELO\"] == best_name][0]\n",
    "\n",
    "# === 3) Reentrenar en TODO el histórico con el mejor modelo ===\n",
    "pipe_final = Pipeline([(\"prepro\", prepro), (\"model\", candidatos[best_name])])\n",
    "pipe_final.fit(X, y)\n",
    "\n",
    "# === 4) Guardar artefactos (sobrescribe los viejos) ===\n",
    "joblib.dump(pipe_final, ARTI / \"modelo_boyaca.pkl\")\n",
    "joblib.dump(feature_cols, ARTI / \"feature_cols.pkl\")\n",
    "\n",
    "meta = {\n",
    "    \"model\": type(pipe_final.named_steps[\"model\"]).__name__,\n",
    "    \"n_obs\": int(len(y)),\n",
    "    \"features\": feature_cols,\n",
    "    \"metricas_test_snapshot\": {\n",
    "        \"R2\":   float(df_resultados.iloc[0][\"R2\"]),\n",
    "        \"RMSE\": float(df_resultados.iloc[0][\"RMSE\"]),\n",
    "        \"MAE\":  float(df_resultados.iloc[0][\"MAE\"]),\n",
    "    },\n",
    "    \"source\": \"artifacts/dataset_modelo.csv\"\n",
    "}\n",
    "with open(ARTI / \"metadata.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# (opcional) Deja a mano estos CSV para el tablero\n",
    "df.to_csv(ARTI / \"dataset_modelo.csv\", index=False)          # ya es el mismo\n",
    "df_resultados.to_csv(ARTI / \"df_resultados.csv\", index=False)\n",
    "\n",
    "print(\"✅ Artefactos actualizados en ./artifacts/:\")\n",
    "print(\"   - modelo_boyaca.pkl\")\n",
    "print(\"   - feature_cols.pkl\")\n",
    "print(\"   - metadata.json\")\n",
    "print(\"   - dataset_modelo.csv\")\n",
    "print(\"   - df_resultados.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a39d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
